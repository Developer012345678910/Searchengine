# Nexora Searchengine ğŸš€ğŸ”

Nexora is a modern, lightweight search frontend powered by a high-performance multithreaded Python web crawler. The project is designed to be straightforward, portable, and self-contained â€” with no Node.js, no build tools, and no heavy server stacks required.

---

âœ¨ Key Points

- **Frontend:** Static HTML / CSS / JS â€” just open `index.html` in your browser to search! ğŸ˜ƒ  
- **Crawler:** Python script (`webcrawler/webcrawler.py`) that optionally generates `crawled_data.json` for search content ğŸ  
- **No Node.js or build tools required**  
- **You need Python to run a local server for best experience** ğŸ–¥ï¸

---

ğŸ¯ Features

- âš¡ Efficient, multithreaded Python crawler (fully configurable)  
- ğŸ“¦ Crawl data stored as JSON for seamless frontend use  
- âœ… Service Worker for offline-ready search & fast performance  
- âœ¨ Modern, responsive UI for local JSON search  
- ğŸ› ï¸ Easy customization: modular frontend & backend

---

ğŸš¦ Quick Start

You do not need to run the crawler to use the search â€” a ready-to-use `crawled_data.json` is included! For most users:

1. Clone the repository:
```bash
git clone https://github.com/Developer012345678910/Searchengine.git
cd Searchengine
```

2. Install Python  
Download and install the newest Python version from [python.org](https://www.python.org/) ğŸ

3. Run a local server:
Start a local server for best results:
```bash
python -m http.server 8000
```

4. Open your browser:  
Visit http://localhost:8000 ğŸŒŸ

If `crawled_data.json` is present, youâ€™ll instantly get search results from that dataset.

---

ğŸ•¹ï¸ Optional: Generate or Update Crawl Data

Use the crawler if you want to create or refresh your own search dataset.

1. Install dependencies:
```bash
pip install -r webcrawler/requirements.txt
```

2. Run the crawler (example):
```bash
python webcrawler/webcrawler.py \
  --start-url https://www.example.com/ \
  --max-pages 50 \
  --json-file crawled_data.json
```

Notes:
- ğŸ›‘ The crawler honors `robots.txt` where possible.  
- âš™ï¸ Running the crawler is optional and intended for dataset updates & development â€” not required for everyday searching.

---

ğŸ—‚ï¸ Project Structure

```
Searchengine/
â”œâ”€â”€ CSS/                   # ğŸ¨ Stylesheets
â”œâ”€â”€ JS/                    # âœ¨ Frontend logic & service worker
â”œâ”€â”€ webcrawler/            # ğŸ Python crawler code
â”‚   â”œâ”€â”€ webcrawler.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ How_to_use.txt
â”œâ”€â”€ index.html             # ğŸ  Main application
â”œâ”€â”€ crawled_data.json      # ğŸ” Example/search data (optional)
â”œâ”€â”€ README.md
â”œâ”€â”€ CONTRIBUTING.md
â””â”€â”€ LICENSE.md
```

---

ğŸ¤ Contributing

PRs and ideas are welcome! ğŸ™Œ If your PR changes or adds crawl data, please document which seed URL(s) and crawler options you used. See [CONTRIBUTING.md](https://github.com/Developer012345678910/Searchengine?tab=contributing-ov-file#contributing-to-nexora-searchengine) for details.

---

ğŸ“„ License

MIT License â€” see [LICENSE.md](https://github.com/Developer012345678910/Searchengine?tab=MIT-1-ov-file)
